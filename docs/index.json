[{"content":"","date":null,"permalink":"https://jmj4.github.io/blog/","section":"Jimmy's Notebook","summary":"","title":"Jimmy's Notebook"},{"content":"","date":null,"permalink":"https://jmj4.github.io/blog/topics/nlp/","section":"Topics","summary":"","title":"NLP"},{"content":"","date":null,"permalink":"https://jmj4.github.io/blog/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"https://jmj4.github.io/blog/topics/","section":"Topics","summary":"","title":"Topics"},{"content":"LLM으로 본 의정사태 언론 보도 #의정사태란 2024년 2월부터 약 1년 반에 걸쳐 이어진\n정부와 의료계 간의 갈등을 말합니다.\n이러한 갈등은 사회 전반으로 확산되었고,\n25년 지금까지도 완전히 해소되었다고 보기는 어렵습니다.\n정치적 이해관계가 복잡하게 얽혀 있는 만큼,\n분석 주제로 다루기에도 까다로웠는데요.\n그래서 이번 글에서는 어떤 주장이 옳은지에 대한 이야기는 잠시 접어두고,\n제가 수행한 LLM 기반 텍스트 분석 작업의 기술적인 내용에 집중해 보려고 합니다.\n대한의사협회에 다녀왔습니다 1. 개요 #먼저, 이번 연구는 깊이 있는 결론을 도출하기 위한 본 분석이라기보다는,\n해당 주제를 텍스트 분석 기법으로 풀어낼 수 있을까?\n를 확인하기 위한 탐색적 분석에 가까웠습니다.\n따라서 발표 당시에는 어떤 정량적 결과보다는,\n아래 내용을 위주로 보였습니다.\n의정 사태와 관련된 언론 보도 경향 확인 시도하려는 분석 기법의 직관적인 설명 제시 본격적인 분석과 해석은 발표 이후에 진행하기로 해서요.\n따라서 이번 글 역시 중간 점검에 가깝습니다.\n2. 관련 연구 #분석 방법을 고민하며 해당 연구를 참고했어요.\n이 연구는 간호법 관련 뉴스 기사를 수집한 뒤 LDA를 통해 언론이 어떤 이슈를 중심으로 보도했는지를 분석합니다.\n다만 LDA는 잘 알려진 한계들이 있죠.\n문서 코퍼스를 Bag of words로 처리 단어의 순서나 문맥을 고려하지 않음 쉽게 말해,\n전체 문서를 단어별로 잘라 한 가방에 섞어 넣고\n어떤 단어가 많이 나오는지를 보는 방식입니다\n따라서 의정사태처럼 맥락, 뉘앙스가 중요한 주제에는 바로 적용하기 어렵다고 생각했어요.\n3. 수행 방안 #이러한 한계를 보완하기 위해 BERTopic을 사용했습니다.\n내부적으로 SBERT를 사용하고 있어서 단어 단위가 아닌 문맥을 기반으로 토픽을 생성할 수 있거든요.\n여기에 개별 뉴스의 감성을 확인하기 위해\nLLM을 활용한 감성 라벨링을 함께 진행했습니다.\n즉, 이번 분석은 다음과 같이 진행됐어요.\n뉴스 기사 스크래핑 LLM 기반 감성 라벨링 토픽 모델링 4. 실험 방법 #4.1 데이터 수집 #의정사태가 본격적으로 언급되기 시작한 2024년 1월부터 2024년 12월까지의 뉴스기사를 수집했습니다.\n검색 조건은 다음과 같이 걸었어요.\n본문에 \u0026ldquo;의료\u0026rdquo; 포함 아래 키워들 중 하나 이상 포함 의정사태 의료개혁 의대정원 의대증원 전처리 과정을 거친 후, 총 10,233건의 뉴스 기사를 분석 대상으로 확보했습니다.\n4.2 감성 라벨링 #감성 라벨링에는 gpt-oss 20B 모델을 사용했어요.\n25년 8월에 나온 모델인데요, 모델 사이즈가 13 Gb 정도 되서\nSingle GPU 환경에서 충분히 구동할 수 있었습니다.\n20B 모델의 성능은 o3‑mini와 비슷한 것으로 보고됐어요.\n25년 12월 기준 LMArena에서 140등 정도 하는 모델입니다.\n아무래도 고성능의 제품을 사용하는 것이 좋겠지만, 어디까지나 예비 실험이라 모델 부분은 크게 고려하지 않았습니다.\ngpt-oss의 아키텍쳐를 간단히 살펴보면\nRMSNorm 사용 RoPE 적용 GQA 적용 및 Sliding window attention 사용 MoE 적용 및 SwiGLU module 사용 등 기존 LLM들과 구조 면에서 큰 차이는 없는 것으로 보입니다.\n저는 Ollma와 LangChain을 사용해서 작업을 진행했어요.\n그리고 본문에 긍정적 / 부정적인 단어의 포함 여부가 아닌\n기사 전체가 전달하는 뉘앙스를 판단하도록 만들기 위해\n프롬프트에 \u0026ldquo;공정한 언론인\u0026quot;이란 설명을 주었어요.\n결과적으로 10,233건의 뉴스 기사에 대해\n긍정 / 중립 / 부정 감성 라벨링을 수행하는 데 약 8시간이 걸렸습니다.\n5. 결과 #분석은 예상대로 중립 기사가 가장 많았고, 그 다음으로 부정적인 기사가 많았습니다.\n특히 부정적인 기사는 긍정적인 기사보다 두 배 이상 많이 나타났어요.\n아래 그림은 긍정적인 기사를 1점, 중립적인 기사를 0점, 부정적인 기사를 -1점으로 두고\n월별로 평균을 냈을 때 추이를 나타낸 그래프인데요,\n시간이 지남에 따라 감성이 전체적으로 우하향하는 모습을 볼 수 있었습니다.\n전체 뉴스 감성의 시간적 변화 6. 결론 #해당 발표를 준비하며, 개인적으로 다음과 같은 질문들이 고민이 많이 되었어요.\n1) 뉴스 기사를 분석 대상으로 삼는 것이 과연 적절할까?\n뉴스는 기본적으로 객관성을 지향합니다.\n그렇다면 감성을 분석하는 것이 의미가 있을까요?\n하지만 동시에 뉴스는\n어떤 사실을 선택하고 어떤 순서로 배치하며 어떤 프레임으로 서술할지를 결정합니다 이 과정 자체가 이미 누군가의 해석의 결과물이라는 점에서,\n감성이 반영되어 있지 않다고 말하기도 어렵다고 생각해요.\n감성 분석 연구를 살펴보면 대부분 댓글이나 사용자 피드백과 같이\n주관적인 감성이 충분히 녹아있는 데이터를 분석 대상으로 삼는 걸 볼 수 있었습니다.\n2) LLM이 판단한 감성은 타당할까?\nLLM은 부정적인 기사는 판단할 때\n갈등, 위기, 대립을 강조하는 톤에 집중합니다.\n그렇다면 이 감성은 기자의 감정일까요,\n아니면 독자가 받아들이게 될 정서일까요?\n또 만약 어떤 독자는 해당 기사를 긍정적으로,\n또 다른 독자는 해당 기사를 부정적으로 받아들인다면\n해당 기사의 감성은 어떻게 평가해야 할까요?\n따라서 사회 현상에 관한 문제는\n아직 단일 모델이나 단일 데이터셋으로는 해결하기 어렵다고 느꼈습니다.\n이번 경험을 통해 실제 현장의 목소리를 들을 수 있는 계기가 되어 좋았어요.\n또한, 텍스트 분석을 통해 지난 1년 반 동안 우리가\n어떤 보도를 반복해서 접해왔는지 기술적으로 살펴볼 수 있었어요.\n특히 LLM은 문서의 의도를 파악할 수 있어 복잡한 사회 현상을 관찰하는 데 좋은 도구라고 생각합니다.\n다음 번에는 사용하는 모델과 시스템 프롬프트를 더 고도화할 계획이에요.\n특히 gpt-oss는 Reasoning effort를 시스템 프롬프트로 줄 수 있는데요,\n이를 적용해 보는 것도 재밌을 것 같습니다.\n","date":"2025년 12월 20일","permalink":"https://jmj4.github.io/blog/posts/post2/","section":"Posts","summary":"","title":"의정사태에 나타난 언론보도 경향 탐색"},{"content":"","date":null,"permalink":"https://jmj4.github.io/blog/topics/programming/","section":"Topics","summary":"","title":"Programming"},{"content":"의료 영상 데이터는 주로 DICOM 파일 형태로 저장되는데요,\n이때 수검자 메타데이터가 이미지 상에 함께 나타납니다.\n이를 연구 목적으로 외부로 반출하기 위해서는 이미지 상에 나타나는 개인 정보의 마스킹이 필수적인데요, 이 과정에서 대용량의 데이터를 효율적으로 처리하기 위해 병렬 처리를 고민했습니다.\n이번 글에서는 그 과정을 공유하고자 합니다.\nBrain MRI DICOM 파일 예시 [출처] 1. 상황 설명 #MRI 데이터는 상단 예시와 같이 주로 DICOM 파일 형태로 저장됩니다.\n이때 환자 당 보통 백 장 내외의 사진을 촬영하기 때문에 기간에 따라 대략 수백만 건의 파일을 마스킹해야 하는 상황이 발생합니다.\n2. 문제 정의 #해당 작업은 서버에서 파일을 읽는 I/O Bound 작업과\n파일을 마스킹하는 CPU Bound 작업이 혼합되어 있다고 판단했습니다.\n따라서 ThreadPool을 사용하여 파일을 읽어온 후, 그 결과를 ProcessPool을 사용하여 각 CPU가 마스킹을 수행하는 프로세스를 계획했습니다.\n2-1. 스토리지에서 DICOM 파일 읽기 #MRI 데이터는 PACS 서버에 저장되어 있었기 때문에 데이터를 불러올 때마다 서버에 접근해야 했습니다.\n이때 필연적으로 응답 대기 시간이 발생했는데요,\n읽어올 파일 수가 많을 경우 이러한 대기 시간이 병목으로 작용했습니다.\n이를 해결하기 위해 먼저 멀티스레딩을 떠올렸습니다.\n여러 개의 스레드가 동시에 요청을 보내면, 어떤 스레드가 네트워크 응답을 기다리는 동안 다른 스레드가 다시 새로운 파일을 읽어올 것이므로 응답 대기 시간을 상쇄할 수 있을 것이라고 생각했습니다.\n즉, 동시에 여러 파일을 읽는 데 효과적일 것이라고 판단했습니다.\n2-2. DICOM 파일 마스킹하기 #문제는 멀티스레딩으로 파일을 읽어온 다음이었습니다.\n저는 파이썬의 pydicom 라이브러리를 활용하여 마스킹 작업을 수행했는데요,\n파이썬에서는 GIL(Global Interperter Lock) 때문에 마스킹과 같은 CPU Bound 작업은 멀티스레딩이 큰 효과를 보지 못합니다.\n여기서 GIL이란 파이썬에서 한 번에 하나의 스레드만 실행하도록 하는 일종의 안전장치인데요,\n위와 같이 서버에서 파일을 읽거나, HTTP 요청을 보내는 등 I/O 대기 중에는 GIL이 풀리기 때문에 스레딩 효과를 볼 수 있었죠.\n하지만 CPU 연산 같은 경우에는 GIL이 걸리기 때문에 스레드 기반 병렬화가 효과가 없습니다.\n따라서 아예 프로세스를 늘리는 멀티프로세싱이 필요했습니다.\n이를 통해 읽어온 파일들을 여러 프로세스에 분배하여 작업 속도를 개선할 수 있을 것이라 판단했습니다.\n2-3. 작업 배치 처리하기 #또 한가지 문제는 처리할 파일 수가 매우 많다는 것이었습니다.\n수백만 건의 파일을 처리하고 있는데, 이러한 작업들을 전부 큐에 쌓아두게 된다면 메모리를 GB 단위로 차지하게 되겠죠.\n따라서 OOM 에러를 방지하기 위해 모든 파일을 한번에 제출하는 것이 아닌, 작업을 배치 단위로 나눠서 제출하도록 했습니다.\n3. 결과 및 한계점 #이번 대규모 DICOM 파일 마스킹 사례는 스레드 결과가 프로세스 결과에 전달되는 구조였습니다.\nPACS 서버에서 데이터를 읽어오기 때문에 I/O Bound의 특성이 있었고, 읽어온 파일을 마스킹하는 과정에서 CPU Bound의 특성이 있었기에, 멀티스레딩 그리고 멀티프로세싱 두 방식의 차이 및 적용 방안을 고려하는 계기가 되었습니다.\n나아가 대규모 파일을 처리하기 위해 배치 단위로 작업을 나누고, 메모리를 폭증시키지 않는 선에서 적절한 배치 사이즈를 찾기 위해 여러 테스트를 수행했습니다.\n결과적으로, 이전 방식대로였다면 3개월 이상이 걸릴 작업량을 개발 포함 한달 이내에 완수할 수 있었습니다.\n몇 가지 한계도 존재하는데요,\n대역폭에 한계가 있어요\nPACS 서버의 대역폭이 이미 포화 상태라면 멀티스레딩의 효과가 적을 수 있습니다. 프로세스 간 메모리 복제 비용이 발생해요\n멀티프로세싱은 각 프로세스가 독립적인 메모리를 사용합니다. 현재는 서버로부터 파일을 읽은 뒤, DICOM 파일 객체를 각 프로세스에 전달하고 있는데, 이 과정에서 직렬화/역직렬화 비용이 발생합니다. 파일의 수가 많고 고해상도일수록 이러한 오버헤드는 커질 것입니다. 리소스 관리가 어려워요\n메모리가 제한된 환경에서 최적의 처리 단위 크기나 스레드, 프로세스 수를 구하기가 어려웠습니다. 추가로, python3.13t 에서 GIL을 비활성화하는 옵션이 실험적으로 제공된다고 합니다.\n아직까지는 싱글 스레드에서 성능 하락이 있는 등 한계가 있는 것 같습니다만, 만약 이번 사례에서 GIL이 없었다면 멀티프로세싱 부분을 스레드 기반으로 단순화할 수 있었겠네요.\n또한, 프로세스를 생성 및 종료하는 비용도 줄였을 테고요.\n향후에는 병렬 처리 효율을 높이는 방안을 고민해 봐야겠습니다.\n","date":"2025년 10월 1일","permalink":"https://jmj4.github.io/blog/posts/post1/","section":"Posts","summary":"","title":"멀티스레딩과 멀티프로세싱: 대규모 DICOM 파일 마스킹 방안"}]